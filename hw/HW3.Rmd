---
title: "Homework 3"
author: "Neal Marquez"
date: "October 12, 2018"
output: pdf_document
---

```{R loads, message=F, warning=F}
rm(list=ls())
library(tidyverse)
library(latex2exp)
library(boot)
library(glmnet)
library(plotROC)
library(lmtest)
library(mvtnorm)

cyDF <- read_csv("https://faculty.washington.edu/cadolph/mle/cyyoung.csv") %>%
    mutate(id=1:n())
```

# Question 1

The dataset `cyyoung.csv` contains information on selected North American baseball pitchers from 1980 to 2002. Pitchers’ performance can be measured in several ways: their record of games won or lost, the number of runs (points) they allowed the other team to score per game, the number of players they "struck out," the number of players they "walked," and the number of innings they pitched. At the end of the season, two pitchers (one from the American League, and one from the National League) are voted the best pitchers of the year.

## Section A

Fit a logistic regression to the variable `cy` with `era` and `winpct` as the only covariates. Report the estimated parameters, their standard errors, and the log likelihood at its maximum. Perform this fit using `optim()`, then replicate thefit using `glm()`.

```{R, message=F, warning=F}
evalParams <- function(params){
    X <- cbind(rep(1, nrow(cyDF)), as.matrix(select(cyDF, era, winpct)))
    p <- inv.logit(X %*% params)
    y <- cyDF$cy
    -sum(log((1-p)^(1-y) * p^y))
}

fog <- cy ~ era + winpct
runOPT <- optim(c(0, 0, 0), evalParams, hessian=T)
runGLM <- glm(fog, data=cyDF, family=binomial)

vcovMat <- solve(runOPT$hessian)
se <- sqrt(diag(vcovMat))

bind_rows(
    tibble(coeff=names(runGLM$coefficients), est=runOPT$par) %>%
        mutate(`97.5 %`=est + se*1.96, `2.5 %`=est - se*1.96) %>%
        mutate(Approach="Optim"),
    as_tibble(confint(runGLM)) %>%
        mutate(est=runGLM$coefficients, coeff=names(runGLM$coefficients)) %>%
        mutate(Approach="GLM")) %>%
    filter(coeff!="(Intercept)") %>%
    ggplot(aes(x=coeff, y=est, ymin=`2.5 %`, ymax=`97.5 %`, color=coeff)) +
    geom_point() +
    geom_errorbar(width=.3) +
    facet_wrap(~Approach, nrow=2) +
    theme_classic() +
    guides(color=FALSE) +
    coord_flip() +
    labs(x="Coefficient", y="Estimate")

```

## Section B

Without using a special package like `simcf`, calculate the probability a pitcher receives the Cy Young award given `era`$=\{1.50, 1.75, 2.00, \dots,4.75,5.00 \}$ with `winpct` held at its mean value. Now, calculate the probability again, for the same range of `era`’s, given either `winpct`= 0.5 or `winpct`= 0.9. You should end up with $3\times15=45$ probabilities. Plot these estimated probabilities nicely (the tile package works well for this graphic and the next problem, but for this part, even a matrix works well).

```{R}
expand.grid(era=seq(1.5, 5, by=.25), winpct=c(mean(cyDF$winpct), .5, .9)) %>%
    mutate(prob=inv.logit(predict(runGLM, newdata=.))) %>%
    mutate(winpct=as.factor(round(winpct, 2))) %>%
    ggplot(aes(x=era, y=prob, group=winpct, color=winpct)) +
    geom_point() +
    theme_classic() +
    labs(x="ERA", y="Probability", color="Win %")
```

## Section C

Calculate or simulate 95 percent confidence intervals for each of the probabilities plotted in b. (You may now use any package you wish.) Design a graphic (not a table) to incorporate these confidence intervals. Interpret your findings.

```{R}
expand.grid(era=seq(1.5, 5, by=.25), winpct=c(mean(cyDF$winpct), .5, .9)) %>%
    cbind(as.data.frame(predict(runGLM, newdata=., se.fit=T)[1:2])) %>%
    mutate(lwr=fit-1.96*se.fit, upr=fit+1.96*se.fit) %>%
    mutate_at(c("fit", "lwr", "upr"), inv.logit) %>%
    mutate(winpct=as.factor(round(winpct, 2))) %>%
    ggplot(
        aes(x=era, y=fit, group=winpct, color=winpct, ymin=lwr, ymax=upr,
            fill=winpct)) +
    geom_line() +
    geom_ribbon(alpha=.3) +
    theme_classic() +
    labs(x="ERA", y="Probability", color="Win %") +
    guides(fill=FALSE)
```

## Section D

Find a "better model" of `cy`. You may add other variables from the dataset, remove variables already in the model, and/or transform or any variables you wish, except `cy`. Whatever choice you make you should justify in some fashion. Fit your new model, and show whether your fit has improved using __(i)__ a likelihood ratio test, __(ii)__ AIC and/or BIC, __(iii)__ in-sample ROC curves, __(iv)__ in-sample Actual versus Predicted plots, and __(v)__ cross-validation using the metric(s) of your choice.

```{R}
fupdate <- cy ~ era * year + winpct
updateGLM <- glm(fupdate, data=cyDF, family=binomial)

lrtest(runGLM, updateGLM) %>% 
    mutate(AIC=sapply(list(runGLM, updateGLM), AIC))

predDF <- rbind(
    data.frame(
        predictor = runGLM$fitted.values,
        known.truth = cyDF$cy,
        Model = "Original"),
    data.frame(
        predictor = updateGLM$fitted.values,
        known.truth = cyDF$cy,
        Model = "Updated"))

predDF %>%
    ggplot(aes(d = known.truth, m = predictor, color = Model)) + 
    geom_roc(n.cuts = 0) +
    theme_classic()

predDF %>%
    mutate(CY=known.truth == 1) %>%
    ggplot(aes(x=predictor, group=CY, fill=CY)) +
    geom_histogram(alpha=.3, position="identity", bins=30) +
    theme_classic() +
    facet_wrap(~Model) +
    geom_vline(xintercept=.5, linetype=3)
```

```{R cv}

set.seed(12345)
M <- 100
trainSize <- .7

sapply(1:M, function(x){
    trainDF <- sample_frac(cyDF, size=trainSize) %>%
        mutate(trainID=1:n())
    testDF <- select(trainDF, id, trainID) %>%
        right_join(cyDF, by="id") %>%
        filter(is.na(trainID))

    trainOGGLM <- glm(fog, data=trainDF, family=binomial)
    trainUPGLM <- glm(fupdate, data=trainDF, family=binomial)
    
    predOG <- predict(trainOGGLM, newdata=testDF, type="response")
    predUP <- predict(trainUPGLM, newdata=testDF, type="response")
    res <- c(
        sqrt(sum((testDF$cy - predOG)^2)), 
        sqrt(sum((testDF$cy - predUP)^2)))
    res}) %>%
    t %>%
    as.data.frame %>%
    rename(Original=V1, Updated=V2) %>%
    summary
```

## Section E

Suppose your model from d. has the following form:  
$$
\text{cy}_i \sim \text{Bernoulli}(\pi) 
$$
$$
\pi_i = \text{logit}^{-1}(\alpha + z_i \gamma + \boldsymbol{x}_i \boldsymbol{\beta})
$$
where $z_i$ is a covariate of particular interest and $x_i$ is a vector of additional covariates. We are interested in understanding how the conditional expectation of `cy` changes as $z$such that $z$ chnages from $z_{pre}$ to $z_{post}$. Then simulate the first difference in the probability (or, if you prefer, the relative risk) of receiving a Cy Young given the change in $z$, holding other covariates $\boldsymbol{x}$ constant (that is, make sure $\boldsymbol{x}_\text{pre} =\boldsymbol{x}\text{post}$). Display your results neatly in a plot or table, and be sure to show the uncertainty in these estimates.

```{R}
M <- 1000
betaDraws <- t(rmvnorm(M, updateGLM$coefficients, vcov(updateGLM)))

expand.grid(era=c(2, 3), year=1980:2002, winpct=mean(cyDF$winpct)) %>%
    mutate(`(Intercept)`=1, `era:year`=era*year) %>%
    select(`(Intercept)`, era, year, winpct, `era:year`) %>%
    cbind(as.matrix(.) %*% betaDraws) %>%
    gather(key="simulation", value="value", `1`:`100`) %>%
    mutate(simulation=as.numeric(simulation), value=inv.logit(value)) %>%
    arrange(simulation, year, era) %>%
    group_by(year, simulation) %>%
    summarize(diff=nth(value, 1) - nth(value, 2)) %>%
    summarize(
        mu=mean(diff), 
        lwr=quantile(diff, probs=.025),
        upr=quantile(diff, probs=.975)) %>%
    ggplot(aes(x=year, y=mu, ymin=lwr, ymax=upr)) +
    geom_line(linetype=2) +
    geom_ribbon(alpha=.3) +
    theme_classic() +
    labs(y=TeX("$\\Delta$ Probability"), x="Year") +
    ggtitle("Change in Probability from Decreasing ERA from 3 to 2.")
```

## Section F  
Does logistic regression offer a defensible probability model here? What assumptions of this model might be violated by the variable `cy`?

## Section G  
Suppose the pitchers selected for inclusion in the dataset were all considered "contenders" for the Cy Young award by knowledgable experts. Pitchers whom the experts considered unlikely to win the award were excluded. How might this fact affect your findings?